=== Job started at Fri Feb 27 11:23:44 PM EST 2026 ===
Running on node: babel-p9-20
2026-02-27 23:23:58.808	babel-p9-20:0	train:41	INFO	CLI environment prepared
2026-02-27 23:24:00.376	babel-p9-20:0	py.warnings:109	WARNING	/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)

2026-02-27 23:24:00.956	babel-p9-20:0	train:80	INFO	Configuration:
2026-02-27 23:24:00.957	babel-p9-20:0	train:81	INFO	TrainConfig(run_name='multitask_train', seed=6198, epoch=None, dry_run=False, model=ModelConfig(d_model=3584, n_heads=28, n_kv_heads=4, qkv_bias=True, clip_qkv=None, n_layers=28, mlp_ratio=4, mlp_hidden_size=37888, activation_type='swiglu', block_type='sequential', block_group_size=1, rope=True, rope_full_precision=True, rope_theta=1000000.0, vision_backbone=VisionBackboneConfig(image_model_type='openai', image_default_input_size=(336, 336), image_patch_size=14, image_pos_patch_size=14, image_emb_dim=1024, image_num_heads=16, image_num_key_value_heads=16, image_num_layers=23, image_head_dim=64, image_mlp_dim=4096, image_mlp_activations='quick_gelu', image_dropout_rate=0.0, image_num_pos=577, image_norm_eps=1e-05, attention_dropout=0.0, residual_dropout=0.0, initializer_range=0.02, fsdp_wrap=False, resize_mode='default'), vit_load_path='/data/hf_cache/molmo/pretrained_image_encoders/vit-l-14-336.pt', llm_load_path='/data/hf_cache/molmo/pretrained_llms/qwen2-7b.pt', low_cpu_fsdp=True, attention_type='sdpa', float32_attention=True, attention_dropout=0.0, attention_layer_norm=False, residual_dropout=0.0, response_residual_dropout=0.1, embedding_dropout=0.0, layer_norm_type='rms', layer_norm_with_affine=True, layer_norm_eps=1e-06, attention_layer_norm_with_affine=True, max_sequence_length=4096, max_position_embeddings=None, include_bias=False, bias_for_layer_norm=None, scale_logits=False, vocab_size=152064, embedding_size=152064, additional_vocab_size=128, new_embedding_init_range=0.02, weight_tying=False, init_device=None, init_fn='normal', init_std=0.02, init_cutoff_factor=None, norm_after=False, precision='amp_bf16', max_crops=12, crop_mode='overlap-and-resize-c2', use_col_tokens=True, prompt_type='none', system_prompt_kind='style_and_length', message_formatting='none', always_start_with_space=True, multi_annotation_weighting=None, default_inference_len=65, overlap_margins=[4, 4], pad_value=0.0, image_padding_embed='pad_and_partial_pad', fix_image_padding=True, vit_layers=(-2, -9), image_pooling_h=2, image_pooling_w=2, image_pooling_2d='attention_meanq', image_projector='mlp', image_feature_dropout=0.0, initializer_range=0.02, normalize_input_embeds=False, use_position_ids=True, head_dim=None, tokenizer=TokenizerConfig(identifier='Qwen/Qwen2-7B', tokenizer_dir=None), pad_tokenizer=True, moe_num_experts=8, moe_top_k=2, moe_mlp_impl='sparse', moe_log_expert_assignment=False, moe_shared_expert=False, moe_lbl_in_fp32=False, moe_interleave=False, moe_loss_weight=0.1, moe_zloss_weight=None, moe_dropless=True, moe_capacity_factor=1.25), allow_resume=False, ft_llm=True, ft_vit=True, ft_connector=True, ft_embedding='lm_head', optimizer=OptimizerConfig(name='adamw', learning_rate=0.0001, weight_decay=0.01, betas=(0.9, 0.95), eps=1e-05, connector_learning_rate=0.0002, vit_learning_rate=6e-06, llm_learning_rate=2e-05, connector_weight_decay=0.0, vit_weight_decay=0.0, llm_weight_decay=0.0, connector_betas=(0.9, 0.95), vit_betas=(0.9, 0.95), llm_betas=(0.9, 0.95), connector_eps=1e-06, vit_eps=1e-06, llm_eps=1e-06, metrics_log_interval=20), scheduler=SchedulerConfig(name='multimodal', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, connector_t_warmup=200, vit_t_warmup=2000, llm_t_warmup=2000, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=0.0), data=DataConfig(dataset='pixmo_cap_with_transcripts', mixture=None, root_size_mixture=None, split='train', seed=95818, shuffle_messages=False, pad='to_max', sequence_length=2304, shuffle=True, for_inference=False, multi_modal='torch', num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=False, timeout=0), restore_dataloader=True, fast_forward_batches=None, evaluators=[DatasetEvaluatorConfig(label='val', data=DataConfig(dataset='pixmo_cap_with_transcripts', mixture=None, root_size_mixture=None, split='validation', seed=None, shuffle_messages=False, pad='to_max', sequence_length=2304, shuffle=False, for_inference=False, multi_modal='torch', num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), device_eval_batch_size=None, subset_num_batches=64, max_examples=None, max_new_tokens=448, mm_evaluator=None, save_dir=None, save_to_checkpoint_dir=False, eval_name=None, skip_if_metrics_cached=True), DatasetEvaluatorConfig(label='caption_val', data=DataConfig(dataset='pixmo_cap', mixture=None, root_size_mixture=None, split='validation', seed=None, shuffle_messages=False, pad='to_max', sequence_length=2304, shuffle=False, for_inference=False, multi_modal='torch', num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=None, persistent_workers=True, timeout=0), device_eval_batch_size=None, subset_num_batches=64, max_examples=None, max_new_tokens=448, mm_evaluator=None, save_dir=None, save_to_checkpoint_dir=False, eval_name=None, skip_if_metrics_cached=True)], eval_interval=1000, inf_eval_interval=-1, inf_evaluators=[], save_folder='/data/user_data/aarusha/molmo/checkpoints/captioner', remote_save_folder=None, canceled_check_interval=50, save_interval=4000, save_interval_unsharded=19428, save_interval_ephemeral=None, save_num_checkpoints_to_keep=1, save_num_unsharded_checkpoints_to_keep=-1, save_overwrite=True, force_save_unsharded=False, no_pre_train_checkpoint=True, initial_model_checkpoint=None, load_model_config=None, load_path=None, load_path_sharded_checkpointer=None, reset_optimizer_state=False, reset_trainer_state=False, save_dataloader_state=False, reset_dataloader_state=False, sharded_checkpointer='torch_legacy', max_duration=19428, global_train_batch_size=128, device_train_batch_size=16, device_train_microbatch_size=4, device_eval_batch_size=4, eval_subset_num_batches=-1, eval_on_load=False, device_inf_eval_batch_size=16, inf_eval_subset_num_batches=-1, device_train_grad_accum=4, max_grad_norm=1.0, multi_component_grad_norm=True, batch_divisor='global_batch', max_grad_norm_ratio=None, precision='amp_bf16', wandb=None, speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=20, gen1_gc_interval=1, compile=None, fsdp=FSDPConfig(use_orig_params=True, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, wrapping_strategy='by_block_and_size', precision='float', hybrid_sharding_num_model_replicas=None), softmax_auxiliary_loss=True, softmax_auxiliary_loss_scale=0.0001, time_limit=None, extra_steps_after_cancel=10, python_profiling=False, torch_profiling=False, stop_at=19428, stop_after=None, activation_checkpointing='whole_layer', fused_loss=None)
2026-02-27 23:24:00.957	babel-p9-20:0	train:106	INFO	Saving config to /data/user_data/aarusha/molmo/checkpoints/captioner/config.yaml
2026-02-27 23:24:01.085	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-7B/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-27 23:24:01.102	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-7B/453ed1575b739b5b03ce3758b23befdb0967f40e/config.json "HTTP/1.1 200 OK"
2026-02-27 23:24:01.143	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-7B/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-27 23:24:01.144	babel-p9-20:0	huggingface_hub.utils._http:779	WARNING	Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
2026-02-27 23:24:01.160	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-7B/453ed1575b739b5b03ce3758b23befdb0967f40e/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-27 23:24:01.198	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-27 23:24:01.235	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-27 23:24:01.662	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B "HTTP/1.1 200 OK"
2026-02-27 23:24:01.694	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-7B/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-27 23:24:01.711	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-7B/453ed1575b739b5b03ce3758b23befdb0967f40e/config.json "HTTP/1.1 200 OK"
2026-02-27 23:24:01.746	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-7B/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-27 23:24:01.763	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-7B/453ed1575b739b5b03ce3758b23befdb0967f40e/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-27 23:24:01.795	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-27 23:24:01.804	babel-p9-20:2	huggingface_hub.utils._http:779	WARNING	Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
2026-02-27 23:24:01.832	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-27 23:24:02.254	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B "HTTP/1.1 200 OK"
2026-02-27 23:24:02.315	babel-p9-20:0	root:101	INFO	Padding tokenizer with 418 tokens
2026-02-27 23:24:02.344	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-7B/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-27 23:24:02.361	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-7B/453ed1575b739b5b03ce3758b23befdb0967f40e/config.json "HTTP/1.1 200 OK"
2026-02-27 23:24:02.386	babel-p9-20:7	huggingface_hub.utils._http:779	WARNING	Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
2026-02-27 23:24:02.401	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/Qwen/Qwen2-7B/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-27 23:24:02.418	babel-p9-20:0	httpx:1025	INFO	HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen2-7B/453ed1575b739b5b03ce3758b23befdb0967f40e/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-27 23:24:02.451	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-27 23:24:02.488	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-27 23:24:02.513	babel-p9-20:3	huggingface_hub.utils._http:779	WARNING	Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
2026-02-27 23:24:02.887	babel-p9-20:0	httpx:1025	INFO	HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2-7B "HTTP/1.1 200 OK"
2026-02-27 23:24:02.932	babel-p9-20:0	root:79	INFO	Loading eval dataset: pixmo_cap_with_transcripts/validation
2026-02-27 23:24:02.959	babel-p9-20:0	root:79	INFO	Loading eval dataset: pixmo_cap/validation
2026-02-27 23:24:24.549	babel-p9-20:0	train:161	INFO	Freezing LLM: wte.embedding
2026-02-27 23:24:24.550	babel-p9-20:0	olmo.util:329	INFO	Found local cache of qwen2-7b.pt at /data/hf_cache/molmo/pretrained_llms/qwen2-7b.pt
2026-02-27 23:24:51.568	babel-p9-20:0	olmo.util:329	INFO	Found local cache of vit-l-14-336.pt at /data/hf_cache/molmo/pretrained_image_encoders/vit-l-14-336.pt
2026-02-27 23:24:52.428	babel-p9-20:0	train:191	INFO	Total number of parameters: 8,021,025,280
2026-02-27 23:24:52.429	babel-p9-20:0	train:192	INFO	Number of non-embedding parameters: 7,475,569,152
2026-02-27 23:24:52.430	babel-p9-20:0	train:195	INFO	Peak GPU Memory (MB) before FSDP: 7
2026-02-27 23:24:52.430	babel-p9-20:0	train:198	INFO	Wrapping model with FDSP...
2026-02-27 23:24:58.002	babel-p9-20:0	train:252	INFO	Peak GPU Memory (MB) after FSDP: 8182
2026-02-27 23:24:58.002	babel-p9-20:0	train:253	INFO	Model:
2026-02-27 23:24:58.002	babel-p9-20:0	train:254	INFO	FullyShardedDataParallel(
  (_fsdp_wrapped_module): Molmo(
    (transformer): ModuleDict(
      (wte): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Embedding()
      )
      (emb_drop): Dropout(p=0.0, inplace=False)
      (ln_f): FullyShardedDataParallel(
        (_fsdp_wrapped_module): RMSLayerNorm()
      )
      (blocks): ModuleList(
        (0-27): 28 x FullyShardedDataParallel(
          (_fsdp_wrapped_module): OLMoSequentialBlock(
            (dropout): Dropout(p=0.0, inplace=False)
            (act): SwiGLU()
            (attn_out): Linear(in_features=3584, out_features=3584, bias=False)
            (ff_out): Linear(in_features=18944, out_features=3584, bias=False)
            (rotary_emb): RotaryEmbedding()
            (attn_norm): RMSLayerNorm()
            (ff_norm): RMSLayerNorm()
            (att_proj): Linear(in_features=3584, out_features=4608, bias=True)
            (ff_proj): Linear(in_features=3584, out_features=37888, bias=False)
          )
        )
      )
      (ff_out): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Linear(in_features=3584, out_features=152064, bias=False)
      )
    )
    (vision_backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): MolmoVisionBackbone(
        (image_pooling_2d): ViTMultiHeadDotProductAttention(
          (wq): Linear(in_features=2048, out_features=1024, bias=True)
          (wk): Linear(in_features=2048, out_features=1024, bias=True)
          (wv): Linear(in_features=2048, out_features=1024, bias=True)
          (wo): Linear(in_features=1024, out_features=1024, bias=True)
          (residual_dropout): Dropout(p=0.0, inplace=False)
        )
        (image_projector): ImageProjectorMLP(
          (w1): Linear(in_features=1024, out_features=18944, bias=False)
          (w2): Linear(in_features=18944, out_features=3584, bias=False)
          (w3): Linear(in_features=1024, out_features=18944, bias=False)
          (act): LlamaSwiGLU()
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (image_feature_dropout): Dropout(p=0.0, inplace=False)
        (image_vit): FullyShardedDataParallel(
          (_fsdp_wrapped_module): VisionTransformer(
            (patch_embedding): Linear(in_features=588, out_features=1024, bias=False)
            (pre_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (transformer): BlockCollection(
              (resblocks): ModuleList(
                (0-22): 23 x FullyShardedDataParallel(
                  (_fsdp_wrapped_module): ResidualAttentionBlock(
                    (attention): ViTMultiHeadDotProductAttention(
                      (wq): Linear(in_features=1024, out_features=1024, bias=True)
                      (wk): Linear(in_features=1024, out_features=1024, bias=True)
                      (wv): Linear(in_features=1024, out_features=1024, bias=True)
                      (wo): Linear(in_features=1024, out_features=1024, bias=True)
                      (residual_dropout): Dropout(p=0.0, inplace=False)
                    )
                    (feed_forward): ViTMLP(
                      (w1): Linear(in_features=1024, out_features=4096, bias=True)
                      (act): QuickGELUActivation()
                      (w2): Linear(in_features=4096, out_features=1024, bias=True)
                    )
                    (attention_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                    (ffn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
            )
          )
        )
      )
    )
  )
)
2026-02-27 23:24:58.065	babel-p9-20:0	olmo.optim:1026	INFO	Constructing optimizer with 6 param groups
2026-02-27 23:24:58.066	babel-p9-20:0	train:337	INFO	Starting training...
2026-02-27 23:24:58.068	babel-p9-20:0	olmo.train:1121	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=8,182
2026-02-27 23:25:38.339	babel-p9-20:2	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 2 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 2 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-27 23:25:38.384	babel-p9-20:7	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 7 has a total capacity of 44.39 GiB of which 4.00 GiB is free. Including non-PyTorch memory, this process has 40.38 GiB memory in use. Of the allocated memory 36.63 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 7 has a total capacity of 44.39 GiB of which 4.00 GiB is free. Including non-PyTorch memory, this process has 40.38 GiB memory in use. Of the allocated memory 36.63 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-27 23:25:38.396	babel-p9-20:0	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 0 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 0 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-27 23:25:38.472	babel-p9-20:3	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 3 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 3 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-27 23:25:38.514	babel-p9-20:4	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 4 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 4 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-27 23:25:38.538	babel-p9-20:1	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 1 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 1 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-27 23:25:38.552	babel-p9-20:6	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 6 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 6 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-27 23:25:38.942	babel-p9-20:5	olmo.util:174	CRITICAL	Uncaught OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 5 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/aarusha/molmo/launch_scripts/train_captioner.py", line 210, in <module>
    train(cfg)
  File "/home/aarusha/molmo/scripts/train.py", line 338, in main
    trainer.fit()
  File "/home/aarusha/molmo/olmo/train.py", line 1408, in fit
    metrics = self.train_step(batch, reduce_global_loss=should_log_this_step)
  File "/home/aarusha/molmo/olmo/train.py", line 923, in train_step
    ce_batch_loss, z_batch_loss, batch_accuracy, lb_batch_loss, moe_z_batch_loss, expert_assignments = self.train_batch(batch)
  File "/home/aarusha/molmo/olmo/train.py", line 905, in train_batch
    loss.backward()
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/_tensor.py", line 630, in backward
    torch.autograd.backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 364, in backward
    _engine_run_backward(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB. GPU 5 has a total capacity of 44.39 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.13 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 3.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
