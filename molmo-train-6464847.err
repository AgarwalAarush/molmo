W0227 23:06:14.584000 2354623 site-packages/torch/distributed/run.py:852] 
W0227 23:06:14.584000 2354623 site-packages/torch/distributed/run.py:852] *****************************************
W0227 23:06:14.584000 2354623 site-packages/torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0227 23:06:14.584000 2354623 site-packages/torch/distributed/run.py:852] *****************************************
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
[rank0]:[W227 23:06:25.352425568 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
[rank1]:[E227 23:16:51.711534557 ProcessGroupNCCL.cpp:688] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
[rank2]:[E227 23:16:51.712240208 ProcessGroupNCCL.cpp:688] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
[rank2]:[E227 23:16:51.716549617 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 2]  failure detected by watchdog at work sequence id: 7 PG status: last enqueued work: 8, last completed work: 6
[rank2]:[E227 23:16:51.716560368 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E227 23:16:51.716557478 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 7 PG status: last enqueued work: 8, last completed work: 6
[rank1]:[E227 23:16:51.716577568 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E227 23:16:51.716616378 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 2] First PG on this rank to signal dumping.
[rank1]:[E227 23:16:51.716680799 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank3]:[E227 23:16:51.821886966 ProcessGroupNCCL.cpp:688] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600002 milliseconds before timing out.
[rank3]:[E227 23:16:51.822493826 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 3]  failure detected by watchdog at work sequence id: 7 PG status: last enqueued work: 8, last completed work: 6
[rank3]:[E227 23:16:51.822503896 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E227 23:16:51.822588888 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 3] First PG on this rank to signal dumping.
[rank4]:[E227 23:16:51.838826698 ProcessGroupNCCL.cpp:688] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600021 milliseconds before timing out.
[rank4]:[E227 23:16:51.839449048 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 4]  failure detected by watchdog at work sequence id: 7 PG status: last enqueued work: 8, last completed work: 6
[rank4]:[E227 23:16:51.839459508 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank4]:[E227 23:16:51.839540549 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 4] First PG on this rank to signal dumping.
[rank3]:[E227 23:16:51.899817836 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 6.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank2]:[E227 23:16:51.899817856 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 6.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E227 23:16:51.899872357 ProcessGroupNCCL.cpp:1825] [PG ID 0 PG GUID 0(default_pg) Rank 0] Observed flight recorder dump signal from another rank via TCPStore.
[rank0]:[E227 23:16:51.900025949 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from  rank 4 and we will try our best to dump the debug info. Last enqueued NCCL work: 5, last completed NCCL work: 5.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank5]:[E227 23:16:51.899976578 ProcessGroupNCCL.cpp:1825] [PG ID 0 PG GUID 0(default_pg) Rank 5] Observed flight recorder dump signal from another rank via TCPStore.
[rank5]:[E227 23:16:51.900268373 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 5] Received a dump signal due to a collective timeout from  rank 4 and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 6.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E227 23:16:51.900924484 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank2]:[E227 23:16:51.900931664 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank5]:[E227 23:16:51.900955304 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 5] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank3]:[E227 23:16:51.900944314 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank6]:[E227 23:16:51.930671251 ProcessGroupNCCL.cpp:688] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
[rank6]:[E227 23:16:51.931292701 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 6]  failure detected by watchdog at work sequence id: 7 PG status: last enqueued work: 8, last completed work: 6
[rank6]:[E227 23:16:51.931309531 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank6]:[E227 23:16:51.931403422 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 6] First PG on this rank to signal dumping.
[rank5]:[E227 23:16:51.934124806 ProcessGroupNCCL.cpp:688] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
[rank5]:[E227 23:16:51.934654725 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 5]  failure detected by watchdog at work sequence id: 7 PG status: last enqueued work: 8, last completed work: 6
[rank5]:[E227 23:16:51.934662625 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank6]:[E227 23:16:51.964222879 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 6] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 6.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank7]:[E227 23:16:51.964371921 ProcessGroupNCCL.cpp:1825] [PG ID 0 PG GUID 0(default_pg) Rank 7] Observed flight recorder dump signal from another rank via TCPStore.
[rank7]:[E227 23:16:51.964530844 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 7] Received a dump signal due to a collective timeout from  rank 5 and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 6.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank6]:[E227 23:16:51.964532884 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 6] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank7]:[E227 23:16:51.964860209 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 7] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank7]:[E227 23:16:51.965290116 ProcessGroupNCCL.cpp:688] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
[rank7]:[E227 23:16:51.965789204 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 7]  failure detected by watchdog at work sequence id: 7 PG status: last enqueued work: 8, last completed work: 6
[rank7]:[E227 23:16:51.965796084 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E227 23:16:52.017655765 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 6.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E227 23:16:52.018092462 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank4]:[E227 23:16:52.104464237 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 4] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 6.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank4]:[E227 23:16:52.104704821 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 4] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank1]:[E227 23:17:51.986456606 ProcessGroupNCCL.cpp:749] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E227 23:17:51.986490706 ProcessGroupNCCL.cpp:763] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E227 23:17:51.988302875 ProcessGroupNCCL.cpp:749] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E227 23:17:51.988330176 ProcessGroupNCCL.cpp:763] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E227 23:17:51.988468748 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1470d1b53fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x146ff4171d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x146ff4176261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x146ff41775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1470d18dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x147111e897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x147111f0e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600015 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1470d1b53fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x146ff4171d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x146ff4176261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x146ff41775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1470d18dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x147111e897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x147111f0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1470d1b53fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x146ff39ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x1470d18dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x147111e897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x147111f0e820 in /lib64/libc.so.6)

[rank6]:[E227 23:17:51.988984826 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x147344d94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x147345f71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x147345f76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x147345f775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1474236dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x147463a897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x147463b0e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600053 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x147344d94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x147345f71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x147345f76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x147345f775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1474236dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x147463a897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x147463b0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x147344d94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x1473457ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x1474236dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x147463a897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x147463b0e820 in /lib64/libc.so.6)

[rank3]:[E227 23:17:52.011666270 ProcessGroupNCCL.cpp:749] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E227 23:17:52.011702890 ProcessGroupNCCL.cpp:763] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E227 23:17:52.012371571 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600002 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1456bff94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x1456c1171d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x1456c1176261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1456c11775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14579e8dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1457dec897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1457ded0e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600002 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1456bff94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x1456c1171d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x1456c1176261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1456c11775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14579e8dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1457dec897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1457ded0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1456bff94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x1456c09ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14579e8dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x1457dec897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x1457ded0e820 in /lib64/libc.so.6)

[rank4]:[E227 23:17:52.012689806 ProcessGroupNCCL.cpp:749] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E227 23:17:52.012718617 ProcessGroupNCCL.cpp:763] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E227 23:17:52.013390067 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600021 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x154f5e394fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x154f5f571d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x154f5f576261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x154f5f5775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x15503ccdbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x15507d0897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x15507d10e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600021 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x154f5e394fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x154f5f571d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x154f5f576261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x154f5f5775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x15503ccdbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x15507d0897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x15507d10e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x154f5e394fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x154f5edddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x15503ccdbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x15507d0897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x15507d10e820 in /lib64/libc.so.6)

[rank2]:[E227 23:17:52.013682002 ProcessGroupNCCL.cpp:749] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E227 23:17:52.013712483 ProcessGroupNCCL.cpp:763] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E227 23:17:52.014376773 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14a6f0994fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14a6f1b71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14a6f1b76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14a6f1b775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14a7cf2dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14a80f6897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14a80f70e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14a6f0994fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14a6f1b71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14a6f1b76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14a6f1b775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14a7cf2dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14a80f6897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14a80f70e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14a6f0994fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x14a6f13ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14a7cf2dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x14a80f6897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x14a80f70e820 in /lib64/libc.so.6)

[rank5]:[E227 23:17:52.448793578 ProcessGroupNCCL.cpp:749] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E227 23:17:52.448832539 ProcessGroupNCCL.cpp:763] [Rank 5] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E227 23:17:52.449503630 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14fd2b794fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14fd2c971d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14fd2c976261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14fd2c9775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14fe0a0dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14fe4a4897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14fe4a50e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14fd2b794fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14fd2c971d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14fd2c976261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14fd2c9775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14fe0a0dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14fe4a4897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14fe4a50e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14fd2b794fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x14fd2c1ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14fe0a0dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x14fe4a4897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x14fe4a50e820 in /lib64/libc.so.6)

[rank7]:[E227 23:17:52.473745638 ProcessGroupNCCL.cpp:749] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E227 23:17:52.473777669 ProcessGroupNCCL.cpp:763] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E227 23:17:52.474453180 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14f2c9953fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14f1ebf71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14f1ebf76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14f1ebf775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14f2c96dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14f309c897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14f309d0e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7, OpType=BROADCAST, NumelIn=544997376, NumelOut=544997376, Timeout(ms)=600000) ran for 600086 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14f2c9953fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14f1ebf71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14f1ebf76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14f1ebf775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14f2c96dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14f309c897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14f309d0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14f2c9953fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x14f1eb7ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14f2c96dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x14f309c897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x14f309d0e820 in /lib64/libc.so.6)

/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0227 23:17:59.361000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 2354704 closing signal SIGTERM
W0227 23:17:59.362000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 2354705 closing signal SIGTERM
W0227 23:17:59.362000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 2354706 closing signal SIGTERM
W0227 23:17:59.362000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 2354707 closing signal SIGTERM
W0227 23:17:59.362000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 2354709 closing signal SIGTERM
W0227 23:17:59.362000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 2354710 closing signal SIGTERM
W0227 23:17:59.362000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 2354711 closing signal SIGTERM
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0227 23:18:00.456000 2354623 site-packages/torch/distributed/elastic/multiprocessing/api.py:984] failed (exitcode: -6) local_rank: 4 (pid: 2354708) of binary: /home/aarusha/miniconda3/envs/molmo/bin/python3.10
Traceback (most recent call last):
  File "/home/aarusha/miniconda3/envs/molmo/bin/torchrun", line 6, in <module>
    sys.exit(main())
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/run.py", line 991, in main
    run(args)
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/run.py", line 982, in run
    elastic_launch(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 170, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 317, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=========================================================
launch_scripts/train_captioner.py FAILED
---------------------------------------------------------
Failures:
[1]:
  time      : 2026-02-27_23:18:00
  host      : babel-t5-32.ib
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2354704)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2354704
[2]:
  time      : 2026-02-27_23:18:00
  host      : babel-t5-32.ib
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 2354705)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2354705
[3]:
  time      : 2026-02-27_23:18:00
  host      : babel-t5-32.ib
  rank      : 2 (local_rank: 2)
  exitcode  : -6 (pid: 2354706)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2354706
[4]:
  time      : 2026-02-27_23:18:00
  host      : babel-t5-32.ib
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 2354707)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2354707
[5]:
  time      : 2026-02-27_23:18:00
  host      : babel-t5-32.ib
  rank      : 5 (local_rank: 5)
  exitcode  : -6 (pid: 2354709)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2354709
[6]:
  time      : 2026-02-27_23:18:00
  host      : babel-t5-32.ib
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 2354710)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2354710
[7]:
  time      : 2026-02-27_23:18:00
  host      : babel-t5-32.ib
  rank      : 7 (local_rank: 7)
  exitcode  : -6 (pid: 2354711)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2354711
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-27_23:17:59
  host      : babel-t5-32.ib
  rank      : 4 (local_rank: 4)
  exitcode  : -6 (pid: 2354708)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2354708
=========================================================
