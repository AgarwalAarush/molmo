W0228 13:49:45.680000 3099879 site-packages/torch/distributed/run.py:852] 
W0228 13:49:45.680000 3099879 site-packages/torch/distributed/run.py:852] *****************************************
W0228 13:49:45.680000 3099879 site-packages/torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0228 13:49:45.680000 3099879 site-packages/torch/distributed/run.py:852] *****************************************
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.
  warnings.warn(message, FutureWarning)
[rank0]:[W228 13:50:01.266474917 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
[rank1]:[E228 14:00:01.595350427 ProcessGroupNCCL.cpp:688] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
[rank1]:[E228 14:00:01.600343900 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank1]:[E228 14:00:01.600356030 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E228 14:00:01.600398357 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank2]:[E228 14:00:01.620167618 ProcessGroupNCCL.cpp:688] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank2]:[E228 14:00:01.620620644 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 2]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank2]:[E228 14:00:01.620627086 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank2]:[E228 14:00:01.620663129 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 2] First PG on this rank to signal dumping.
[rank4]:[E228 14:00:01.625169432 ProcessGroupNCCL.cpp:688] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
[rank5]:[E228 14:00:01.625603216 ProcessGroupNCCL.cpp:688] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
[rank4]:[E228 14:00:01.625653853 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 4]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank4]:[E228 14:00:01.625661709 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank4]:[E228 14:00:01.625715931 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 4] First PG on this rank to signal dumping.
[rank5]:[E228 14:00:01.626041241 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 5]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank5]:[E228 14:00:01.626048081 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank5]:[E228 14:00:01.626093069 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 5] First PG on this rank to signal dumping.
[rank3]:[E228 14:00:01.628826970 ProcessGroupNCCL.cpp:688] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
[rank3]:[E228 14:00:01.629273902 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 3]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank3]:[E228 14:00:01.629286533 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank3]:[E228 14:00:01.629321100 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 3] First PG on this rank to signal dumping.
[rank0]:[E228 14:00:01.632646085 ProcessGroupNCCL.cpp:688] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
[rank0]:[E228 14:00:01.633035148 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank0]:[E228 14:00:01.633040852 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E228 14:00:01.633063379 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 0] First PG on this rank to signal dumping.
[rank7]:[E228 14:00:01.645991882 ProcessGroupNCCL.cpp:688] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600063 milliseconds before timing out.
[rank7]:[E228 14:00:01.646499123 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 7]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank7]:[E228 14:00:01.646506465 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank7]:[E228 14:00:01.646545958 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 7] First PG on this rank to signal dumping.
[rank6]:[E228 14:00:01.668557892 ProcessGroupNCCL.cpp:688] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
[rank6]:[E228 14:00:01.669030565 ProcessGroupNCCL.cpp:2277] [PG ID 0 PG GUID 0(default_pg) Rank 6]  failure detected by watchdog at work sequence id: 1 PG status: last enqueued work: 1, last completed work: -1
[rank6]:[E228 14:00:01.669038420 ProcessGroupNCCL.cpp:735] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank6]:[E228 14:00:01.669081650 ProcessGroupNCCL.cpp:2610] [PG ID 0 PG GUID 0(default_pg) Rank 6] First PG on this rank to signal dumping.
[rank4]:[E228 14:00:02.203978526 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 4] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank2]:[E228 14:00:02.203994217 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E228 14:00:02.203997283 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank5]:[E228 14:00:02.204012602 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 5] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[E228 14:00:02.204015475 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank4]:[E228 14:00:02.205153901 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 4] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank2]:[E228 14:00:02.205159758 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank3]:[E228 14:00:02.205166682 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank5]:[E228 14:00:02.205172598 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 5] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank0]:[E228 14:00:02.205184106 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank7]:[E228 14:00:02.213033770 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 7] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank6]:[E228 14:00:02.213069532 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 6] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank7]:[E228 14:00:02.213180781 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 7] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank6]:[E228 14:00:02.213300974 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 6] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank1]:[E228 14:00:02.260990528 ProcessGroupNCCL.cpp:1890] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 1, last completed NCCL work: -1.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E228 14:00:02.261222864 ProcessGroupNCCL.cpp:1606] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1, only active collectives: 0
[rank2]:[E228 14:01:01.829705372 ProcessGroupNCCL.cpp:749] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E228 14:01:01.829728246 ProcessGroupNCCL.cpp:763] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E228 14:01:01.835554754 ProcessGroupNCCL.cpp:749] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E228 14:01:01.835575281 ProcessGroupNCCL.cpp:763] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E228 14:01:01.836463238 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x152237f94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x152239171d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x152239176261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1522391775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1523168dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x152356c897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x152356d0e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600061 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x152237f94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x152239171d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x152239176261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1522391775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1523168dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x152356c897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x152356d0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x152237f94fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x1522389ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x1523168dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x152356c897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x152356d0e820 in /lib64/libc.so.6)

[rank5]:[E228 14:01:01.836960918 ProcessGroupNCCL.cpp:749] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E228 14:01:01.836976875 ProcessGroupNCCL.cpp:763] [Rank 5] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E228 14:01:01.837842711 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14bd59994fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14bd5ab71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14bd5ab76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14bd5ab775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14be382dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14be786897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14be7870e820 in /lib64/libc.so.6)

[rank7]:[E228 14:01:01.838832747 ProcessGroupNCCL.cpp:749] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E228 14:01:01.838848853 ProcessGroupNCCL.cpp:763] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E228 14:01:01.839818901 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600063 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1507aa153fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x1506cc771d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x1506cc776261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1506cc7775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1507a9edbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1507ea4897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1507ea50e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
[rank2]:[E228 14:01:01.844026099 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1501a0f53fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x1500c3571d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x1500c3576261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1500c35775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1501a0cdbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1501e12897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1501e130e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14bd59994fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14bd5ab71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14bd5ab76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14bd5ab775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14be382dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14be786897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14be7870e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14bd59994fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x14bd5a3ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14be382dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x14be786897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x14be7870e820 in /lib64/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600063 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1507aa153fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x1506cc771d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x1506cc776261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1506cc7775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1507a9edbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1507ea4897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1507ea50e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1507aa153fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x1506cbfddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x1507a9edbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x1507ea4897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x1507ea50e820 in /lib64/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1501a0f53fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x1500c3571d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x1500c3576261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x1500c35775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1501a0cdbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1501e12897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1501e130e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x1501a0f53fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x1500c2dddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x1501a0cdbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x1501e12897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x1501e130e820 in /lib64/libc.so.6)

[rank1]:[E228 14:01:01.846380509 ProcessGroupNCCL.cpp:749] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E228 14:01:01.846381877 ProcessGroupNCCL.cpp:749] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E228 14:01:01.846396026 ProcessGroupNCCL.cpp:763] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E228 14:01:01.846400430 ProcessGroupNCCL.cpp:763] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E228 14:01:01.847333018 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14a6ac194fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14a6ad371d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14a6ad376261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14a6ad3775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14a78aadbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14a7cae897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14a7caf0e820 in /lib64/libc.so.6)

[rank1]:[E228 14:01:01.847353391 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x146167953fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x146089f71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x146089f76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x146089f775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1461676dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1461a7a897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1461a7b0e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600085 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14a6ac194fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14a6ad371d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14a6ad376261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14a6ad3775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14a78aadbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14a7cae897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14a7caf0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14a6ac194fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x14a6acbddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14a78aadbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x14a7cae897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x14a7caf0e820 in /lib64/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600020 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x146167953fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x146089f71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x146089f76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x146089f775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x1461676dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1461a7a897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1461a7b0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x146167953fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x1460897ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x1461676dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x1461a7a897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x1461a7b0e820 in /lib64/libc.so.6)

[rank4]:[E228 14:01:01.848789266 ProcessGroupNCCL.cpp:749] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E228 14:01:01.848805210 ProcessGroupNCCL.cpp:763] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E228 14:01:01.849530204 ProcessGroupNCCL.cpp:749] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E228 14:01:01.849547415 ProcessGroupNCCL.cpp:763] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E228 14:01:01.849780355 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14f7a3353fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14f6c5971d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14f6c5976261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14f6c59775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14f7a30dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14f7e36897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14f7e370e820 in /lib64/libc.so.6)

[rank3]:[E228 14:01:01.850526861 ProcessGroupNCCL.cpp:2093] [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14577c753fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14569ed71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14569ed76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14569ed775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14577c4dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1457bca897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1457bcb0e820 in /lib64/libc.so.6)

terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::DistBackendErrorc10::DistBackendError'
'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600046 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14577c753fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14569ed71d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14569ed76261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14569ed775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14577c4dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x1457bca897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x1457bcb0e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14577c753fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x14569e5ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14577c4dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x1457bca897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x1457bcb0e820 in /lib64/libc.so.6)
  what():  
[PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=1, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:691 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14f7a3353fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x26a (0x14f6c5971d7a in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16a1 (0x14f6c5976261 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0x105 (0x14f6c59775a5 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbad4 (0x14f7a30dbad4 in /lib64/libstdc++.so.6)
frame #5: <unknown function> + 0x897fa (0x14f7e36897fa in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10e820 (0x14f7e370e820 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2099 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x9d (0x14f7a3353fdd in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x9bbbc8 (0x14f6c51ddbc8 in /home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xdbad4 (0x14f7a30dbad4 in /lib64/libstdc++.so.6)
frame #3: <unknown function> + 0x897fa (0x14f7e36897fa in /lib64/libc.so.6)
frame #4: <unknown function> + 0x10e820 (0x14f7e370e820 in /lib64/libc.so.6)

/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
W0228 14:01:04.734000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 3099964 closing signal SIGTERM
W0228 14:01:04.735000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 3099965 closing signal SIGTERM
W0228 14:01:04.735000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 3099967 closing signal SIGTERM
W0228 14:01:04.736000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 3099968 closing signal SIGTERM
W0228 14:01:04.736000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 3099969 closing signal SIGTERM
W0228 14:01:04.736000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 3099970 closing signal SIGTERM
W0228 14:01:04.736000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 3099971 closing signal SIGTERM
/home/aarusha/miniconda3/envs/molmo/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0228 14:01:05.228000 3099879 site-packages/torch/distributed/elastic/multiprocessing/api.py:984] failed (exitcode: -6) local_rank: 2 (pid: 3099966) of binary: /home/aarusha/miniconda3/envs/molmo/bin/python3.10
Traceback (most recent call last):
  File "/home/aarusha/miniconda3/envs/molmo/bin/torchrun", line 6, in <module>
    sys.exit(main())
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/run.py", line 991, in main
    run(args)
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/run.py", line 982, in run
    elastic_launch(
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 170, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/aarusha/miniconda3/envs/molmo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 317, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
launch_scripts/train_captioner.py FAILED
--------------------------------------------------------
Failures:
[1]:
  time      : 2026-02-28_14:01:05
  host      : babel-q5-32.ib
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 3099964)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099964
[2]:
  time      : 2026-02-28_14:01:05
  host      : babel-q5-32.ib
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 3099965)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099965
[3]:
  time      : 2026-02-28_14:01:05
  host      : babel-q5-32.ib
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 3099967)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099967
[4]:
  time      : 2026-02-28_14:01:05
  host      : babel-q5-32.ib
  rank      : 4 (local_rank: 4)
  exitcode  : -6 (pid: 3099968)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099968
[5]:
  time      : 2026-02-28_14:01:05
  host      : babel-q5-32.ib
  rank      : 5 (local_rank: 5)
  exitcode  : -6 (pid: 3099969)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099969
[6]:
  time      : 2026-02-28_14:01:05
  host      : babel-q5-32.ib
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 3099970)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099970
[7]:
  time      : 2026-02-28_14:01:05
  host      : babel-q5-32.ib
  rank      : 7 (local_rank: 7)
  exitcode  : -6 (pid: 3099971)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099971
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-28_14:01:04
  host      : babel-q5-32.ib
  rank      : 2 (local_rank: 2)
  exitcode  : -6 (pid: 3099966)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3099966
========================================================
